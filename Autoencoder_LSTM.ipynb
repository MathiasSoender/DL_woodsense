{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/sensor1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       sensor_id                  timestamp  temperature  humidity  \\\n",
       "0              1  2020-08-28 03:00:00+00:00    15.012970     77.00   \n",
       "1              1  2020-08-28 04:00:00+00:00    15.198558     77.00   \n",
       "2              1  2020-08-28 05:00:00+00:00    15.384146     77.00   \n",
       "3              1  2020-08-28 06:00:00+00:00    15.569734     77.00   \n",
       "4              1  2020-08-28 07:00:00+00:00    15.755322     77.00   \n",
       "...          ...                        ...          ...       ...   \n",
       "57967         42  2020-10-20 19:00:00+00:00    10.695000     99.99   \n",
       "57968         42  2020-10-20 20:00:00+00:00    10.660000     99.99   \n",
       "57969         42  2020-10-20 21:00:00+00:00    10.666667     99.99   \n",
       "57970         42  2020-10-20 22:00:00+00:00    10.673333     99.99   \n",
       "57971         42  2020-10-20 23:00:00+00:00    10.680000     99.99   \n",
       "\n",
       "             ohms   moisture  weather_humidity  weather_pressure  \\\n",
       "0      270.067017  12.990156         95.579013       1004.827315   \n",
       "1      261.323128  13.038244         96.114617       1004.332653   \n",
       "2      252.579239  13.086331         95.666685       1003.678711   \n",
       "3      243.835350  13.134418         94.448831       1003.407445   \n",
       "4      235.091461  13.182506         93.943871       1003.311881   \n",
       "...           ...        ...               ...               ...   \n",
       "57967   55.961068  15.403181         83.363973       1002.234081   \n",
       "57968   56.017153  15.401460         89.845917       1002.017034   \n",
       "57969   55.524990  15.416776         92.324956       1001.807850   \n",
       "57970   55.032826  15.432091         93.548737       1001.611697   \n",
       "57971   54.540663  15.447407         94.581768       1001.269094   \n",
       "\n",
       "       weather_temp_dew  weather_temp_dry  weather_wind_dir  \\\n",
       "0             11.475756         12.131467        108.966365   \n",
       "1             12.136401         12.751486        103.050999   \n",
       "2             12.646634         13.377194        102.429338   \n",
       "3             13.056790         13.834935        111.662148   \n",
       "4             13.650739         14.700486         88.541827   \n",
       "...                 ...               ...               ...   \n",
       "57967          7.042912          9.740160        191.395958   \n",
       "57968          7.713235          9.304126        191.618384   \n",
       "57969          8.107616          9.218550        188.959264   \n",
       "57970          8.526925          9.522056        191.684353   \n",
       "57971          9.127066          9.842974        205.012215   \n",
       "\n",
       "       weather_wind_speed  weather_wind_max  weather_wind_min  \\\n",
       "0                1.825696          2.818346               NaN   \n",
       "1                2.153497          3.074299               NaN   \n",
       "2                1.645717          2.843965               NaN   \n",
       "3                2.000634          3.346310               NaN   \n",
       "4                1.934695          3.406582               NaN   \n",
       "...                   ...               ...               ...   \n",
       "57967            2.868811          5.950202               NaN   \n",
       "57968            2.302741          4.824224               NaN   \n",
       "57969            2.140578          4.475191               NaN   \n",
       "57970            2.046183          4.293633               NaN   \n",
       "57971            2.100424          4.360580               NaN   \n",
       "\n",
       "       weather_precip_past10min  \n",
       "0                      0.000000  \n",
       "1                      0.000000  \n",
       "2                      0.000000  \n",
       "3                      0.000000  \n",
       "4                      0.000000  \n",
       "...                         ...  \n",
       "57967                  0.068418  \n",
       "57968                  0.031582  \n",
       "57969                  0.000000  \n",
       "57970                  0.000000  \n",
       "57971                  0.000000  \n",
       "\n",
       "[57972 rows x 15 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sensor_id</th>\n      <th>timestamp</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ohms</th>\n      <th>moisture</th>\n      <th>weather_humidity</th>\n      <th>weather_pressure</th>\n      <th>weather_temp_dew</th>\n      <th>weather_temp_dry</th>\n      <th>weather_wind_dir</th>\n      <th>weather_wind_speed</th>\n      <th>weather_wind_max</th>\n      <th>weather_wind_min</th>\n      <th>weather_precip_past10min</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2020-08-28 03:00:00+00:00</td>\n      <td>15.012970</td>\n      <td>77.00</td>\n      <td>270.067017</td>\n      <td>12.990156</td>\n      <td>95.579013</td>\n      <td>1004.827315</td>\n      <td>11.475756</td>\n      <td>12.131467</td>\n      <td>108.966365</td>\n      <td>1.825696</td>\n      <td>2.818346</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2020-08-28 04:00:00+00:00</td>\n      <td>15.198558</td>\n      <td>77.00</td>\n      <td>261.323128</td>\n      <td>13.038244</td>\n      <td>96.114617</td>\n      <td>1004.332653</td>\n      <td>12.136401</td>\n      <td>12.751486</td>\n      <td>103.050999</td>\n      <td>2.153497</td>\n      <td>3.074299</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2020-08-28 05:00:00+00:00</td>\n      <td>15.384146</td>\n      <td>77.00</td>\n      <td>252.579239</td>\n      <td>13.086331</td>\n      <td>95.666685</td>\n      <td>1003.678711</td>\n      <td>12.646634</td>\n      <td>13.377194</td>\n      <td>102.429338</td>\n      <td>1.645717</td>\n      <td>2.843965</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2020-08-28 06:00:00+00:00</td>\n      <td>15.569734</td>\n      <td>77.00</td>\n      <td>243.835350</td>\n      <td>13.134418</td>\n      <td>94.448831</td>\n      <td>1003.407445</td>\n      <td>13.056790</td>\n      <td>13.834935</td>\n      <td>111.662148</td>\n      <td>2.000634</td>\n      <td>3.346310</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2020-08-28 07:00:00+00:00</td>\n      <td>15.755322</td>\n      <td>77.00</td>\n      <td>235.091461</td>\n      <td>13.182506</td>\n      <td>93.943871</td>\n      <td>1003.311881</td>\n      <td>13.650739</td>\n      <td>14.700486</td>\n      <td>88.541827</td>\n      <td>1.934695</td>\n      <td>3.406582</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57967</th>\n      <td>42</td>\n      <td>2020-10-20 19:00:00+00:00</td>\n      <td>10.695000</td>\n      <td>99.99</td>\n      <td>55.961068</td>\n      <td>15.403181</td>\n      <td>83.363973</td>\n      <td>1002.234081</td>\n      <td>7.042912</td>\n      <td>9.740160</td>\n      <td>191.395958</td>\n      <td>2.868811</td>\n      <td>5.950202</td>\n      <td>NaN</td>\n      <td>0.068418</td>\n    </tr>\n    <tr>\n      <th>57968</th>\n      <td>42</td>\n      <td>2020-10-20 20:00:00+00:00</td>\n      <td>10.660000</td>\n      <td>99.99</td>\n      <td>56.017153</td>\n      <td>15.401460</td>\n      <td>89.845917</td>\n      <td>1002.017034</td>\n      <td>7.713235</td>\n      <td>9.304126</td>\n      <td>191.618384</td>\n      <td>2.302741</td>\n      <td>4.824224</td>\n      <td>NaN</td>\n      <td>0.031582</td>\n    </tr>\n    <tr>\n      <th>57969</th>\n      <td>42</td>\n      <td>2020-10-20 21:00:00+00:00</td>\n      <td>10.666667</td>\n      <td>99.99</td>\n      <td>55.524990</td>\n      <td>15.416776</td>\n      <td>92.324956</td>\n      <td>1001.807850</td>\n      <td>8.107616</td>\n      <td>9.218550</td>\n      <td>188.959264</td>\n      <td>2.140578</td>\n      <td>4.475191</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57970</th>\n      <td>42</td>\n      <td>2020-10-20 22:00:00+00:00</td>\n      <td>10.673333</td>\n      <td>99.99</td>\n      <td>55.032826</td>\n      <td>15.432091</td>\n      <td>93.548737</td>\n      <td>1001.611697</td>\n      <td>8.526925</td>\n      <td>9.522056</td>\n      <td>191.684353</td>\n      <td>2.046183</td>\n      <td>4.293633</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57971</th>\n      <td>42</td>\n      <td>2020-10-20 23:00:00+00:00</td>\n      <td>10.680000</td>\n      <td>99.99</td>\n      <td>54.540663</td>\n      <td>15.447407</td>\n      <td>94.581768</td>\n      <td>1001.269094</td>\n      <td>9.127066</td>\n      <td>9.842974</td>\n      <td>205.012215</td>\n      <td>2.100424</td>\n      <td>4.360580</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>57972 rows × 15 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df, test_split = 0.2, val_split = 0.2, RANDOM_SEED = 42):\n",
    "    \n",
    "    idx_train, idx_test = train_test_split([i for i in range(1,43)], test_size=test_split, random_state = RANDOM_SEED)\n",
    "    idx_only_train, idx_val = train_test_split(idx_train, test_size=val_split, random_state = RANDOM_SEED)\n",
    "    \n",
    "    df['Train'] = False\n",
    "    df['Validation'] = False\n",
    "    df['Test'] = False\n",
    "    \n",
    "    df.loc[df['sensor_id'].isin(idx_train), 'Train'] = True\n",
    "    df.loc[df['sensor_id'].isin(idx_test), 'Test'] = True\n",
    "    df.loc[df['sensor_id'].isin(idx_val), 'Validation'] = True\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "def set_index(df):\n",
    "    df['idx']=0\n",
    "    for idx, grp in enumerate(df.groupby('sensor_id')):\n",
    "        df.loc[df.sensor_id == grp[0], \"idx\"] = idx\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class data(Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_csv, test, val, list_features,scaling = True, fixed_points = False):\n",
    "        super(data).__init__()\n",
    "        df = pd.read_csv(path_to_csv)\n",
    "        df = split_train_test_val(df)\n",
    "        \n",
    "        if test:\n",
    "            self.df = df[df['Test']==True].reset_index()\n",
    "            del df\n",
    "            set_index(self.df)\n",
    "\n",
    "        else:\n",
    "            if val:\n",
    "                self.df = df[(df['Train']==True) & (df['Validation']==True)].reset_index()\n",
    "            else:\n",
    "                self.df = df[(df['Train']==True) & (df['Validation']==False)].reset_index()\n",
    "                \n",
    "            del df\n",
    "            set_index(self.df)\n",
    "\n",
    "        self.fixed_points = fixed_points\n",
    "        self.list_features = list_features\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.fixed_points:\n",
    "            nb_point=200\n",
    "            start = np.random.randint(0,len(self.df[self.df['idx']==index]) - nb_point +1)\n",
    "            _input = torch.from_numpy(np.array(self.df[self.df['idx']==index][self.list_features][start:start + nb_point]).transpose())[:,:-1]\n",
    "            target = torch.from_numpy(np.array(self.df[self.df['idx']==index][self.list_features][start:start + nb_point]).transpose())[:,1:]\n",
    "            \n",
    "            return _input, target\n",
    "            \n",
    "        else:\n",
    "            _input = torch.from_numpy(np.array(self.df[self.df['idx']==index][self.list_features]).transpose())[:,:-1]\n",
    "            target = torch.from_numpy(np.array(self.df[self.df['idx']==index][self.list_features]).transpose())[:,1:]\n",
    "            \n",
    "            return _input, target\n",
    "     \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df.groupby('sensor_id'))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data('Data/sensor1.csv', test = False, list_features = ['humidity'], fixed_points=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train |  torch.Size([1, 1, 1292]) | tensor([[[77.0000, 77.0000, 77.0000,  ..., 69.6000, 69.8000, 70.0000]]],\n       dtype=torch.float64)\nTest |  torch.Size([1, 1, 1292]) | tensor([[[77.0000, 77.0000, 77.0000,  ..., 69.4000, 69.6000, 69.8000]]],\n       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Shape : [Batch, Features, Time Steps]\n",
    "\n",
    "test, train = next(iter(loader))\n",
    "print(\"Train | \", train.shape, \"|\", train)\n",
    "print(\"Test | \", test.shape, \"|\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 1, 1292])\n(1, 1292)\n"
     ]
    }
   ],
   "source": [
    "# # input: array-like of shape (n_samples, n_features)\n",
    "\n",
    "print(train.shape)\n",
    "train = torch.squeeze(train, 1)\n",
    "test = torch.squeeze(test, 1)\n",
    "\n",
    "# Fit to data to range 0 - 1, then transform it.\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.fit_transform(test)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.unsqueeze(torch.from_numpy(X_train).permute(1,0), 2)\n",
    "X_test = torch.unsqueeze(torch.from_numpy(X_test).permute(1,0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1292, 1, 1])\ntorch.Size([1292, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        input_size = 1\n",
    "        latent_1 = 400\n",
    "        bottleneck = 16\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, latent_1)\n",
    "        self.lstm2 = nn.LSTM(latent_1,bottleneck)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm1(x)\n",
    "        output = self.relu(output)\n",
    "        output, _ = self.lstm2(output)\n",
    "        return output\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        input_size = 1\n",
    "        latent_1 = 400\n",
    "        bottleneck = 16\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(bottleneck, latent_1)\n",
    "        self.lstm2 = nn.LSTM(latent_1, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm1(x)\n",
    "        output = self.relu(output)\n",
    "        output, _ = self.lstm2(output)\n",
    "        return output\n",
    "        \n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x.float())\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Autoencoder(\n  (encoder): Encoder(\n    (lstm1): LSTM(1, 400)\n    (lstm2): LSTM(400, 16)\n    (relu): ReLU()\n  )\n  (decoder): Decoder(\n    (lstm1): LSTM(16, 400)\n    (lstm2): LSTM(400, 1)\n    (relu): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Autoencoder(Encoder(), Decoder())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_reshape(_input, target):\n",
    "    scaler = MinMaxScaler()\n",
    "    _input = torch.squeeze(_input, 1)\n",
    "    target = torch.squeeze(target, 1)\n",
    "    scaler.fit(_input)\n",
    "    _input = scaler.transform(_input)\n",
    "    target = scaler.transform(target)\n",
    "    _input = torch.unsqueeze(torch.from_numpy(_input).permute(1,0), 2)\n",
    "    target = torch.unsqueeze(torch.from_numpy(target).permute(1,0), 2)\n",
    "    \n",
    "    return _input, target\n",
    "\n",
    "def reshape(_input, target):\n",
    "    _input = _input.permute(2,0,1)\n",
    "    target = target.permute(2,0,1)\n",
    "    return _input, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl, n_epochs):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "    # Track loss\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        model = model.train()\n",
    "        for idx, (_input, target) in enumerate(train_dl):\n",
    "            _input, target = scale_and_reshape(_input, target)\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction = model(_input)\n",
    "            loss = criterion(reconstruction, _input)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            print(f\"Epoch : {epoch}, Batch : {idx/len(train_dl)}, Training loss : {loss.item()}\")\n",
    "\n",
    "        model = model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx, (_input, target) in enumerate(val_dl):\n",
    "                _input, target = scale_and_reshape(_input, target)\n",
    "                reconstruction = model(_input)\n",
    "                loss = criterion(reconstruction, _input)\n",
    "                val_losses.append(loss.item())\n",
    "                print(f\"Epoch : {epoch}, Batch : {idx/len(val_dl)}, Validation loss : {loss.item()}\")\n",
    "\n",
    "        train_loss = np.mean(train_loss)\n",
    "        val_loss = np.mean(val_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch : {epoch}, train loss : {train_loss}, val loss :  {val_loss}')\n",
    "    \n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch : 0, Batch : 0.0, Training loss : 0.025147709758989695\n",
      "Epoch : 0, Batch : 0.038461538461538464, Training loss : 0.016847745104017448\n",
      "Epoch : 0, Batch : 0.07692307692307693, Training loss : 0.013234457634846195\n",
      "Epoch : 0, Batch : 0.11538461538461539, Training loss : 0.01032870750487595\n",
      "Epoch : 0, Batch : 0.15384615384615385, Training loss : 0.008829878526158037\n",
      "Epoch : 0, Batch : 0.19230769230769232, Training loss : 0.007754812511846758\n",
      "Epoch : 0, Batch : 0.23076923076923078, Training loss : 0.00653874150406584\n",
      "Epoch : 0, Batch : 0.2692307692307692, Training loss : 0.0053801880149770874\n",
      "Epoch : 0, Batch : 0.3076923076923077, Training loss : 0.004326057934775773\n",
      "Epoch : 0, Batch : 0.34615384615384615, Training loss : 0.0033662546823921605\n",
      "Epoch : 0, Batch : 0.38461538461538464, Training loss : 0.002493217624078405\n",
      "Epoch : 0, Batch : 0.4230769230769231, Training loss : 0.0016956115893448542\n",
      "Epoch : 0, Batch : 0.46153846153846156, Training loss : 0.0009459354741486967\n",
      "Epoch : 0, Batch : 0.5, Training loss : 0.0013634990320541738\n",
      "Epoch : 0, Batch : 0.5384615384615384, Training loss : 0.0015820935456487308\n",
      "Epoch : 0, Batch : 0.5769230769230769, Training loss : 0.0015838739007935758\n",
      "Epoch : 0, Batch : 0.6153846153846154, Training loss : 0.0014345085162451696\n",
      "Epoch : 0, Batch : 0.6538461538461539, Training loss : 0.0011813889403740965\n",
      "Epoch : 0, Batch : 0.6923076923076923, Training loss : 0.000857879632733996\n",
      "Epoch : 0, Batch : 0.7307692307692307, Training loss : 0.0004908370132276053\n",
      "Epoch : 0, Batch : 0.7692307692307693, Training loss : 9.261620385314373e-05\n",
      "Epoch : 0, Batch : 0.8076923076923077, Training loss : 0.00032317498139528504\n",
      "Epoch : 0, Batch : 0.8461538461538461, Training loss : 0.0005824957321871088\n",
      "Epoch : 0, Batch : 0.8846153846153846, Training loss : 0.0007217040801809191\n",
      "Epoch : 0, Batch : 0.9230769230769231, Training loss : 0.0007670648324353725\n",
      "Epoch : 0, Batch : 0.9615384615384616, Training loss : 0.0007369146247251064\n",
      "Epoch : 0, Batch : 3.5714285714285716, Validation loss : 0.0006452532820055683\n",
      "Epoch : 0, Batch : 3.5714285714285716, Validation loss : 0.0006452532820055683\n",
      "Epoch : 0, Batch : 3.5714285714285716, Validation loss : 0.0006452532820055683\n",
      "Epoch : 0, Batch : 3.5714285714285716, Validation loss : 0.0006452532820055683\n",
      "Epoch : 0, Batch : 3.5714285714285716, Validation loss : 0.0006452532820055683\n",
      "Epoch : 0, Batch : 3.5714285714285716, Validation loss : 0.0006452532820055683\n",
      "Epoch : 0, Batch : 3.5714285714285716, Validation loss : 0.0006452532820055683\n",
      "Epoch : 0, train loss : 0.004561821880783507, val loss :  nan\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-2bfa39c133ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/sensor1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'humidity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_dataset = data('Data/sensor1.csv', test = False, val=False, list_features = ['humidity'], fixed_points=False)\n",
    "train_dl = DataLoader(train_dataset, batch_size=1)\n",
    "val_dataset = data('Data/sensor1.csv', test = False, val=True, list_features = ['humidity'], fixed_points=False)\n",
    "val_dl = DataLoader(val_dataset, batch_size=1)\n",
    "model, train_losses, val_losses = train_model(model, train_dl, val_dl, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f45e4997e9699ec145c2f5ae8a0ce7fd5ec7dd7e942db307b6ed1b1b778c9d2"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}